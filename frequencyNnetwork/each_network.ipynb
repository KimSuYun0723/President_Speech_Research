{"cells":[{"cell_type":"markdown","metadata":{"id":"AIVuiuTk4sFE"},"source":["# 대통령 연설문 네트워크 분석 코드"]},{"cell_type":"markdown","metadata":{"id":"tdMwMEBj4ui1"},"source":["# 1. 필요한 라이브러리 다운로드 & 설치, 폰트 설정"]},{"cell_type":"markdown","metadata":{"id":"tgMRjfP746R7"},"source":["### 필요한 라이브러리 설치"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":39882,"status":"ok","timestamp":1724550139334,"user":{"displayName":"‍김수윤[재학 / ELLT학과]","userId":"03712461850989734479"},"user_tz":-540},"id":"J2dYWol748iS","outputId":"ced07915-67fe-46c3-caa2-f4805ba3cfe9"},"outputs":[],"source":["# NLTK\n","#!pip install nltk\n","import nltk\n","nltk.download('averaged_perceptron_tagger')\n","nltk.download('punkt')\n","\n","# 기타 라이브러리\n","import pandas as pd\n","import numpy as np\n","from IPython.display import display, HTML\n","\n","from collections import defaultdict\n","from collections import Counter\n","\n","# 한글형태소분석기: 키위 형태소분석기 설치\n","# 다양한 한글형태소분석기 비교: https://hipster4020.tistory.com/184\n","#!pip install --upgrade pip\n","#!pip install konlpy\n","#!pip install kiwipiepy  # 키위 형태소분석기 설치\n","\n","from kiwipiepy import Kiwi\n","from kiwipiepy.utils import Stopwords\n","stopwords = Stopwords()                # 불용어 처리시 사용"]},{"cell_type":"markdown","metadata":{"id":"AcvRzYwr5J24"},"source":["### 폰트"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# 시각화 라이브러리: seaborn, matplotlib\n","import seaborn as sns\n","import matplotlib as mpl\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import matplotlib.font_manager as fm\n","\n","# 윈도우 시스템에 있는 Malgun Gothic 폰트를 사용\n","plt.rcParams['font.family'] = 'Malgun Gothic'\n","plt.rcParams['axes.unicode_minus'] = False"]},{"cell_type":"markdown","metadata":{"id":"QXYkb7C25u4R"},"source":["# 2. 빈도분석"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":460},"executionInfo":{"elapsed":1069586,"status":"ok","timestamp":1724556074417,"user":{"displayName":"‍김수윤[재학 / ELLT학과]","userId":"03712461850989734479"},"user_tz":-540},"id":"O0uZFKN95Ys7","outputId":"7118d0d1-eb55-4617-8efd-4ef5a9967197"},"outputs":[],"source":["# ==================== 대통령 이름(for문으로 순회하여 코드 활용 가능)\n","name_file = ['YoonBoSun', 'ParkJeongHee', 'ChoiGyuHa', 'JeonDuHwuan',\n","             'RohTaeWoo', 'KimYoungSam', 'KimDaeJung', 'RohMooHyun', 'LeeMyungBak',\n","             'ParkGeunHye', 'MoonJaeIn', 'YoonSeokYeol']\n","\n","name = \"\" # name_file에서 원하는 대통령 이름을 찾아 넣으십시오.\n","\n","# ==================== 데이터 가져오기\n","file_path = f\"./dataset/cleaned_data/cleaned_{name}.csv\"\n","df_data = pd.read_csv(file_path)\n","\n","# 선택하고자 하는 열 이름들을 리스트로 작성\n","selected_columns = [\"title\", \"speech\"]\n","df = df_data[selected_columns]\n","\n","# ======================================== 전처리\n","# 전처리: 데이터 특수문자 제거\n","df['title_preprocessed'] = df['title'].str.replace('\\n\\n', ' ', regex=True).str.strip()\n","df['title_preprocessed'] = df['title_preprocessed'].str.replace('\\n', ' ', regex=True).str.strip()\n","df['title_preprocessed'] = df['title_preprocessed'].str.replace('\\\\[.*?\\\\]', '', regex=True)\n","df['title_preprocessed'] = df['title_preprocessed'].str.replace('[^\\\\w\\\\s]', '', regex=True) # 특수 문자 삭제\n","df['title_preprocessed'] = df['title_preprocessed'].str.replace('625', '6.25', regex=True)\n","\n","df['speech_preprocessed'] = df['speech'].str.replace('\\n\\n', ' ', regex=True).str.strip()\n","df['speech_preprocessed'] = df['speech_preprocessed'].str.replace('\\n', ' ', regex=True).str.strip()\n","df['speech_preprocessed'] = df['speech_preprocessed'].str.replace('\\\\[.*?\\\\]', '', regex=True)\n","df['speech_preprocessed'] = df['speech_preprocessed'].str.replace('[^\\\\w\\\\s]', '', regex=True) # 특수 문자 삭제\n","df['speech_preprocessed'] = df['speech_preprocessed'].str.replace('625', '6.25', regex=True)\n","\n","\n","# ======================================== 형태소 분석 -> 제목&연설문 키워드\n","# 형태소 분석기 초기화\n","kiwi = Kiwi()\n","\n","# 불용어 목록 정의\n","stopwords = {'와', '이', '에', '을', '고', '있', '습니다', '이'}\n","\n","# 키워드 추출 함수 정의\n","def extract_keywords(text):\n","    analysis = kiwi.analyze(text)\n","    # 분석 결과가 여러 개일 수 있으므로 첫 번째 결과만 사용\n","    res = analysis[0][0]\n","    return [word + ('다' if tag in ['VV', 'VA'] else '')  # 동사와 형용사에 '다' 추가\n","            for word, tag, _, _ in res\n","            if tag in ['NNG', 'NNP', 'VV', 'VA', 'MAG'] and word not in stopwords and len(word) > 1]     # MAG 일반 부  사\n","\n","# 제목/연설문 키워드 추출\n","df['title_keyword'] = df['title_preprocessed'].apply(extract_keywords) # 제목 키워드\n","df['speech_keyword'] = df['speech_preprocessed'].apply(extract_keywords) # 연설문 키워드\n","df['total_keyword'] = df['title_keyword'] + df['speech_keyword'] # 제목 + 연설문 키워드\n","\n","\n","# ======================================== 전체 빈도수 파일로 저장\n","# Counter()로 빈도수 세기\n","title_keywords = [keyword for sublist in df['title_keyword'] for keyword in sublist]\n","speech_keywords = [keyword for sublist in df['speech_keyword'] for keyword in sublist]\n","total_keywords = title_keywords + speech_keywords\n","total_counts = Counter(total_keywords)\n","print(\"전체 토큰 수 :\", sum(total_counts.values())) # 전체 토큰 수\n","\n","# 전체 키워드 파일 다운로드\n","df_total = pd.DataFrame(list(total_counts.items()), columns=['키워드', '빈도수'])\n","df_total = df_total.sort_values(by='빈도수', ascending=False)           # 빈도수 기준으로 정렬\n","df_total.to_csv(f'{name}_total_counts.csv', encoding='utf-8-sig', index=False)  # to_csv()로\n","\n","# 데이터프레임을 입력 -> 최빈도 50 키워드 추출 -> 데이터프레임 반환\n","def df_to_50_keywords(df_data):\n","  \"\"\"데이터프레임을 입력으로 하면 최빈도 50 키워드 추출하고 데이터프레임으로 반환하는 함수\"\"\"\n","  keywords = [keyword for sublist in df_data for keyword in sublist]         # keywords 전체 키워드 리스트\n","  counts = Counter(keywords)                                                 # counts 전체 키워드, 빈도수 튜플\n","  top_50_keywords = counts.most_common(50)                                   # 빈도수 상위 50 키워드\n","\n","  # 데이터프레임으로 보기\n","  top_50_keywords = pd.DataFrame(top_50_keywords, columns=['키워드', '빈도수'])\n","  return top_50_keywords\n","\n","# df_to_50_keywords()에 df['제목_키워드']를 입력해서 전체 키워드 빈도수 상위 50 추출\n","title_50_keywords = df_to_50_keywords(df['title_keyword'])\n","speech_50_keywords = df_to_50_keywords(df['speech_keyword'])\n","df_50_keywords = df_to_50_keywords(df['total_keyword']) # 상위 50개 빈도수 키워드/빈도수\n","\n","\n","# ======================================== 빈도수 상위 30개 그래프\n","# 빈도수 데이터 준비: '명사'를 index로, '빈도수'를 values로 하는 Series로 변환\n","series = pd.Series(data = df_50_keywords['빈도수'].values, index= df_50_keywords['키워드'])[:30]\n","\n","# 시각화\n","plt.figure(figsize=(15, 6))\n","ax = sns.barplot(x=series.index, y=series.values, palette='viridis')\n","plt.xlabel('키워드', fontsize = 13)                       # 폰트사이즈 조절\n","plt.ylabel('빈도수', fontsize = 13)                       # 폰트사이즈 조절\n","plt.title('빈도수 상위 30위 키워드', fontsize = 17)       # 폰트사이즈, 그래프 타이틀 적절하게 수정!\n","plt.xticks(rotation=45, fontsize = 12)                    # 폰트사이즈 조절\n","plt.grid(axis='y', linestyle='--', alpha=0.7)\n","\n","# 각 막대 위에 기사 수 표시\n","for p in ax.patches:\n","    height = p.get_height()\n","    ax.annotate(f'{int(height)}',\n","                xy=(p.get_x() + p.get_width() / 2, height),\n","                xytext=(0, 5),  # 5 points vertical offset\n","                textcoords='offset points',\n","                ha='center', va='bottom')"]},{"cell_type":"markdown","metadata":{"id":"aYy3g1fG57In"},"source":["# 3. 연관어 분석"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1,"status":"aborted","timestamp":1724554963725,"user":{"displayName":"‍김수윤[재학 / ELLT학과]","userId":"03712461850989734479"},"user_tz":-540},"id":"ujwdwMOH8SyV"},"outputs":[],"source":["import pandas as pd\n","import networkx as nx\n","from itertools import combinations\n","import re\n","\n","#!pip install kss\n","import kss"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"elapsed":9,"status":"error","timestamp":1724556075611,"user":{"displayName":"‍김수윤[재학 / ELLT학과]","userId":"03712461850989734479"},"user_tz":-540},"id":"T88UsHrb8b2T","outputId":"10f3cebe-bb6b-43d8-a263-b9ac7239985f"},"outputs":[{"ename":"NameError","evalue":"name 'kss' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-7ff7f792f0c0>\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \"\"\"\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mspeech\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mspeeches\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m   \u001b[0msent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit_sentences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspeech\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m   \u001b[0msentences\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'kss' is not defined"]}],"source":["# ======================================== [전처리 안된 문장, ..., 전처리 안된 문장]\n","sentences = [] # 문장 별 리스트\n","\n","title = list(df['title'])\n","sentences.extend(title)\n","speeches = list(df['speech'])\n","\n","# 연설문 문장을 리스트로 저장\n","\"\"\"\n","전처리 하기 전에 문장을 먼저 나눔.\n","sentence splitter가 더 잘 쪼개도록 만들기 위함임.\n","\"\"\"\n","for speech in speeches:\n","  sent = kss.split_sentences(speech)\n","  sentences.extend(sent)\n","\n","\n","# ======================================== [전처리된 문장, ..., 전처리된 문장]\n","cleaned_sentences = [] # 문장 속에서 전처리\n","for sentence in sentences:\n","    # 정규표현식 사용하여 처리\n","    cleaned = re.sub(r'\\n\\n', ' ', sentence).strip()\n","    cleaned = re.sub(r'\\n', ' ', cleaned).strip()\n","    cleaned = re.sub(r'\\\\\\[.*?\\\\\\]', '', cleaned)  # 정규 표현식으로 대괄호 안의 내용을 제거\n","    cleaned = re.sub(r'[^\\w\\s]', '', cleaned)  # 특수 문자 제거\n","    cleaned = cleaned.replace('625', '6.25')  # 특정 문자열은 기본 replace 사용\n","    cleaned_sentences.append(cleaned)\n","\n","\n","# ======================================== [[키워드, ..., 키워드], ..., [키워드, ..., 키워드]]\n","sent_words_list = [] # 문장 별로 토크나이징\n","\n","# 불용어 목록 정의\n","stopwords = {'와', '이', '에', '을', '고', '이'}\n","\n","# 명사 추출 함수 정의\n","def extract_nouns(text):\n","    analysis = kiwi.analyze(text)\n","    # 분석 결과가 여러 개일 수 있으므로 첫 번째 결과만 사용\n","    res = analysis[0][0]\n","    return [word for word, tag, _, _ in res\n","            if tag in ['NNG', 'NNP'] and word not in stopwords and len(word) > 1]     # MAG 일반 부사\n","\n","# 문장 단위로 토크나이징\n","for sentence in cleaned_sentences:\n","  sent_words_list.append(extract_nouns(sentence))\n","\n","# 비어있는 리스트 제거 후 --> 최종 키워드 이중 리스트\n","final_sentences = [sentence for sentence in sent_words_list if sentence]\n","\n","\n","# ======================================== 인접 행렬 생성\n","G = nx.Graph()\n","\n","# 모든 행에 대해 반복\n","for sent_word in final_sentences:\n","    # 문장 단위로 바이그램 생성 (같은 토큰이 쌍을 이루지 않도록)\n","    bigrams = [combo for combo in combinations(sent_word, 2) if combo[0] != combo[1]]\n","    for bigram in bigrams:\n","        if G.has_edge(*bigram):\n","            G[bigram[0]][bigram[1]]['weight'] += 1\n","        else:\n","            G.add_edge(bigram[0], bigram[1], weight=1)\n","\n","# 자기 자신과의 연결(루프) 제거\n","G.remove_edges_from(nx.selfloop_edges(G))\n","\n","# 네트워크 행렬 생성\n","adj_matrix = nx.to_pandas_adjacency(G, weight='weight')\n","\n","# 전체 명사, 빈도수 튜플을 딕셔너리로 변경\n","frequency = dict(total_counts)\n","\n","# 빈도수 상위 200개 단어 선택\n","sorted_frequency = sorted(frequency.items(), key=lambda x: x[1], reverse=True)[:200]\n","top_200_words = [word for word, freq in sorted_frequency]\n","\n","# top_200_words 리스트와 인접 행렬의 인덱스를 교차하여 실제 존재하는 단어만 선택\n","existing_words = [word for word in top_200_words if word in adj_matrix.index]\n","\n","# 인접 행렬에서 존재하는 단어들만 선택\n","filtered_adj_matrix = adj_matrix.loc[existing_words, existing_words]\n","\n","# 인접 행렬을 CSV 파일로 저장\n","filtered_adj_matrix.to_csv(f'{name}_matrix_top200.csv', encoding='utf-8-sig', index=True)   # 한글 깨짐 방지: encoding='utf-8-sig' 추가\n","files.download(f'{name}_matrix_top200.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":5,"status":"aborted","timestamp":1724556075612,"user":{"displayName":"‍김수윤[재학 / ELLT학과]","userId":"03712461850989734479"},"user_tz":-540},"id":"V68ZOzs1EWWE"},"outputs":[],"source":["# 네트워크 필터링: 엣지의 weight 값이 300 이상인 경우만 사용\n","filtered_edges = [(u, v) for (u, v, d) in G.edges(data=True) if d['weight'] >= 0]\n","\n","# 필터링된 노드들\n","filtered_nodenames = []\n","for u, v, d in G.edges(data=True):\n","    if d['weight'] >= 0:\n","        filtered_nodenames.append(u)\n","        filtered_nodenames.append(v)\n","\n","filtered_nodenames = list(set(filtered_nodenames))\n","\n","# 필터링된 서브그래프 생성\n","H = G.edge_subgraph(filtered_edges).copy()\n","\n","# Degree Centrality 계산\n","degree_centrality = nx.degree_centrality(H)\n","node_size = [degree_centrality[node] * 10000 for node in H]\n","\n","# 노드와 엣지 정보 엑셀 파일로 저장\n","# 노드 리스트 생성 (노드 이름과 중심성)\n","nodes_data = {\n","    'Id': list(H.nodes()),\n","    'Degree Centrality': [degree_centrality[node] for node in H.nodes()]\n","}\n","nodes_df = pd.DataFrame(nodes_data)\n","nodes_df.to_excel(f'{name}_nodes.xlsx', index=False)  # 노드 데이터를 엑셀로 저장\n","\n","# 엣지 리스트 생성 (출발 노드, 도착 노드, 가중치)\n","edges_data = {\n","    'Source': [u for u, v in filtered_edges],\n","    'Target': [v for u, v in filtered_edges],\n","    'Weight': [G[u][v]['weight'] for u, v in filtered_edges]\n","}\n","edges_df = pd.DataFrame(edges_data)\n","edges_df.to_excel(f'{name}_edges.xlsx', index=False)  # 엣지 데이터를 엑셀로 저장\n","\n","files.download(f'{name}_edges.xlsx')\n","files.download(f'{name}_nodes.xlsx')\n","\n","print(\"노드와 엣지 데이터를 각각 'nodes.xlsx'와 'edges.xlsx' 파일로 저장했습니다.\")"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyPdYPmlYy5RiGjz+9SO2U25","provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.7"}},"nbformat":4,"nbformat_minor":0}
